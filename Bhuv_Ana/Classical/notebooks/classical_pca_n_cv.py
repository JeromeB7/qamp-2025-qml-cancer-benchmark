# -*- coding: utf-8 -*-
"""classical_pca n cv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O9CG2O2DJ-bMg8FLLXtHkdpLvxUJFBD9
"""

from sklearn.decomposition import PCA
# Basic numerical & plotting libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Machine learning
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    balanced_accuracy_score,
    confusion_matrix
)

# Reproducibility
np.random.seed(42)

# Load Breast Cancer Wisconsin (Diagnostic) dataset
data = load_breast_cancer()

X = data.data
y = data.target

feature_names = data.feature_names
target_names = data.target_names

# Train-test split (fixed for reproducibility)
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# Standardize features
scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

def evaluate_model(model, X_train, X_test, y_train, y_test, model_name="Model"):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    bal_acc = balanced_accuracy_score(y_test, y_pred)

    print(f"\n{model_name}")
    print(f"Accuracy          : {acc:.4f}")
    print(f"F1 Score          : {f1:.4f}")
    print(f"Balanced Accuracy : {bal_acc:.4f}")

    return {
        "Model": model_name,
        "Accuracy": acc,
        "F1": f1,
        "Balanced Accuracy": bal_acc
    }

pca_dims = [5, 10, 15, 20]
pca_results = []

for k in pca_dims:
    print(f"\n--- PCA with {k} components ---")

    pca = PCA(n_components=k, random_state=42)

    X_train_pca = pca.fit_transform(X_train_scaled)
    X_test_pca = pca.transform(X_test_scaled)

    # Linear SVM
    linear_pca = SVC(kernel="linear", C=1.0)
    pca_results.append(
        evaluate_model(
            linear_pca,
            X_train_pca,
            X_test_pca,
            y_train,
            y_test,
            model_name=f"Linear SVM + PCA ({k})"
        )
    )

    # RBF SVM
    rbf_pca = SVC(kernel="rbf", C=1.0, gamma="scale")
    pca_results.append(
        evaluate_model(
            rbf_pca,
            X_train_pca,
            X_test_pca,
            y_train,
            y_test,
            model_name=f"RBF SVM + PCA ({k})"
        )
    )

pca_results_df = pd.DataFrame(pca_results)
pca_results_df

plt.figure(figsize=(6,4))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.xlabel("Number of PCA Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("PCA Explained Variance")
plt.grid(True)
plt.tight_layout()
plt.savefig("pca_explained_variance.png", dpi=300)
plt.show()

from sklearn.model_selection import StratifiedKFold, cross_val_score

# Define 10-fold stratified CV
cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Create DataFrame for easier analysis
df = pd.DataFrame(X_train_scaled, columns=feature_names)

# Compute correlation matrix
corr_matrix = df.corr()

# Upper triangle of correlation matrix
upper_triangle = corr_matrix.where(
    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
)

# Correlation threshold
corr_threshold = 0.9

# Columns to drop
to_drop = [
    column for column in upper_triangle.columns
    if any(upper_triangle[column] > corr_threshold)
]

print("Original feature count:", len(feature_names))
print("Features to drop:", len(to_drop))
print(to_drop)

# Convert scaled arrays to DataFrame for pruning
X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)
X_test_df = pd.DataFrame(X_test_scaled, columns=feature_names)

# Drop correlated features
X_train_reduced = X_train_df.drop(columns=to_drop).values
X_test_reduced = X_test_df.drop(columns=to_drop).values

linear_svm_cv = SVC(kernel="linear", C=1.0)

# F1 score CV
f1_scores = cross_val_score(
    linear_svm_cv,
    X_train_reduced,
    y_train,
    cv=cv,
    scoring="f1"
)

# Balanced accuracy CV
bal_acc_scores = cross_val_score(
    linear_svm_cv,
    X_train_reduced,
    y_train,
    cv=cv,
    scoring="balanced_accuracy"
)

print("10-Fold CV Results (Linear SVM – Pruned Features)")
print(f"F1 Score           : {f1_scores.mean():.4f} ± {f1_scores.std():.4f}")
print(f"Balanced Accuracy  : {bal_acc_scores.mean():.4f} ± {bal_acc_scores.std():.4f}")

